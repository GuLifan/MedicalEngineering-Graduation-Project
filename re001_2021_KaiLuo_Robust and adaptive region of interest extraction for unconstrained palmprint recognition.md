#### LIFANGU ME_GP Reference_001
### 无约束掌纹识别中的鲁棒自适应感兴趣区域提取
#### Robust and adaptive region of interest extraction for unconstrained palm-print recognition_May11,2021_IF1.0
#### 1st_罗凯, CA_钟德星
#### DOI: 10.1117/1.JEI.30.3.033005
_________________
_________________
### 阅读总结
[工作] 采集了角度无约束的数据集
[方法] tiny-YOLOv3 检测手掌    
    MobilenetV2和辅助训练网络（角度分类训练）实现关键点捕捉14个点划定范围和建立坐标系，画出ROI    
    使用Meta完成ROI的匹配测试
_________________
_________________
[0_Abstract]()

[背景]() 无约束条件下的掌纹感兴趣区域提取是实现掌纹识别不可回避的问题。<br/>
[困难]() 然而，手掌大小、姿态、光照和背景的多样性无疑构成了巨大的挑战。<br/>
[实践]() 使用5部手机，在闪光灯开启和关闭的情况下，从100人中收集了约3万张无约束掌纹图像;逐张14个关键点人工标注，构成XJTU-UP数据集。<br/>
[成果]() 一种掌纹ROI的提取方法: <br/>
<ol>
    <li>首先手掌检测,去除无关背景</li>
    <li>检测关键点</li>
    <li>最后基于所获得的点建立坐标系以提取感兴趣区域</li>
    <li>在关键点检测过程中，引入了辅助网络和数据不平衡功能，提高了检测精度。</li>
</ol>
在XJTU-UP上，与次优方法相比，当误接受率为0.01%时，识别准确率最高提高了2.16%，真接受率最高提高了20.56%。

[keywords]() ROI; 手掌检测: 关键点检测
_________________
[1_Introduction]() 

[掌纹为什么可以用于生物识别]()
<ol>
    <li>掌纹图像包含了许多区别性信息，如脊线和掌纹，这可以确保鲁棒性和识别准确性。</li>
    <li>具有多生物特征识别的潜力，可以与其他基于手的特征（指纹、指关节、手腕、手背静脉）一起使用。</li>
    <li>手掌图像采集的成本很低，可以通过移动的等便携式设备完成。</li>
</ol>

[掌纹识别的ROI]()
掌纹特征——手指末端到手腕的手掌区域，最突出的是三条主线和几条皱纹。
由于大小和形状不一样，存在大量冗余信息，不适合使用整个区域。
因此选择手掌中心包含三条主线的正方形区域作为感兴趣区域（ROI）

[掌纹识别的工作流程]()
一个完整的掌纹识别系统由两个阶段组成：登记阶段和识别阶段。
第一阶段主要是建立一个登记集，将个人掌纹和身份信息绑定。
第二阶段包括掌纹图像摄取、图像预处理、特征提取和匹配四个部分。
预处理的一个重要目的是获得稳定ROI，这对掌纹识别系统的准确性至关重要。需要无约束方法适应现实。

[作者的工作]()
<ol>
    <li>XJTU-UP数据集</li>
    <li>提出了一种鲁棒的轻量级感兴趣区域提取模型，该模型由手掌检测和关键点检测两个模块组成。</li>
    <li>引入辅助网络来提供手掌角的估计，提高收敛速度和精度。采用专用的损失函数来缓解手掌角度差异引起的类别不平衡问题</li>
</ol>

_________________
[2_Related Works]()

[数据集]() 完全无约束的数据库，手的姿势、采集的设备、采集的场景都由用户自己选择，比如NUIGP 、MPD、XJTU-UP

| 数据集类型 |    名称     |  采集场景   |
|:------|:---------:|:-------:|
| 约束数据库 |   IIT-D   |   箱子    |
| 约束数据库 |   HFUT    |  黑色背景   |
| 部分非约束 | 11k Hands |  白色背景   |
| 非约束   |    MPD    | 室内无规定背景 |
| 非约束   |  XJTU-UP  | 室内无规定背景 |

![img.png](images/img001_001.png)

[经典的ROI提取方法]() 
检测手掌上的谷点（指缝根部）建立坐标系，然后根据一定的经验参数提取ROI。
主要区别在于获得谷点的方法。标准的ROI提取方法过程是首先通过大津阈值法或肤色模型将手掌与背景分离，然后执行测量以确定界标。通常用于约束数据库。
将原始图像转换为具有阈值的二值图像后，利用掌纹中两个间隙之间的切线来提取食指和无名指之间的谷点。

[深度学习提取ROI的方法]()
<ol>
    <li>
        Bao和Guo首先使用浅层网络来识别手掌图像是左手还是右手。<br/>
        然后，他们设计了另一个网络来检测三个山谷点（就是指根缝隙）的坐标。<br/>
        后一种网络在前一种网络的基础上增加了一个卷积层，以增加输入图像的大小，将更多的像素级特征引入神经网络。
    </li>
    <li>
        Izadpanahkakhk等人提出了另一种类似的方法，但是它输出ROI区域的角点的坐标、宽度和高度。<br/>
        上述两种方法都只在受约束的数据库上进行了实验，例如PolyU或CASIA。
    </li>
    <li>
    Liu和Kumar在无约束的室内和室外场景中拍摄了一系列视频，然后从视频中获得不同的帧，经过数据增强后形成数据库。<br/>
    在这个数据库的基础上，他们训练了一个快速RCNN模型来直接检测ROI。<br/>
    然而，他们只使用mAP和召回率两个指标来衡量检测模型的性能，而没有使用相应的识别或验证指标来衡量所获得的ROI。<br/>
    最近，Zhang等人提出通过 Tiny-YOLOv 3检测两个双指间隙和手掌中心。双指间隙是包含两个谷点的矩形区域。<br/>
    根据检测到的两个双指间隙和手掌中心建立坐标系，提取感兴趣区域。虽然该方法在无约束数据库（MPD）上取得了很好的性能.<br/>
    但仍有一些不足之处：<br/>
        （1）MPD数据库中的手掌在角度和形状上不够多样。手掌处于水平或垂直状态，手指分离;<br/>
        （2）手指间隙的检测容易受到手掌角度和形状的影响，当手掌倾斜时，直接检测以获得ROI的策略可能不起作用。
    </li>
</ol>

_________________
[3_Methods]()

每部(5)手机，每位志愿者在闪光灯打开和关闭的情况下，拍摄了大约15张左右手的图像。因此，数据库中有10个不同的域，总计约30,000张图像。
每幅图片的14个关键点进行了人工标注，包括手掌轮廓边缘的谷点和其他点。
MPD是另一个面向解决无约束条件下掌纹识别问题的数据库，提出了一个包括ROI提取和ROI匹配的系统DeepMPV+。
![img.png](images/img001_002.png)

[作者的方案]()
<ol>
    <li>通过对象检测输出手掌区域，同时去除无关背景。</li>
    <li>通过回归模型估计关键点。</li>
    <li>根据获得的关键点建立坐标系，提取感兴趣区域。</li>
</ol>
本文采用Tiny-YOLOv3来实现手掌检测。
与DeepMPV+不同，我们检测整个手掌，而DeepMPV+需要检测双指间隙和手掌中心(图6)。

[关键点检测]()
关键点定位算法实质上是在像素空间中寻找特定的极值点。堆叠的卷积层来提取深度特征，并最终通过全连接层获得坐标。
损失函数采用每个点的估计值和GT之间的均方误差。
一般来说，网络层越深，点预测的效果越好，推理时间越长。因此，骨干的选择是精确度和效率之间的权衡。
作为一种轻量级网络，由于其独特的设计，MobilenetV2在计算机视觉的所有领域都表现出了优越的性能。
与常规褶积相比，MobilenetV2区块的纵向可分离褶积大大减少了LOPS。同时，由于线性瓶颈和倒残差的结构，模型的性能也具有可比性。
因此，在我们提出的方法中，使用MobilenetV2块来获得14个点的坐标作为主干，而不是传统的卷积运算。
最终的全连通层的输入是通过对不同大小的三个卷积层的输出特征进行全局平均汇集和拼接来获得的。

受前人工作的启发，在原有网络的基础上引入了一个辅助网络，目的是估计手掌的角度。
辅助网络将Mobilenetv2模块输出的特征作为输入，相当于在主干上增加了一个分支。
该分支由卷积层和完全连通层组成，并输出估计的角度。
由辅助网络估计的这个角度将与它的GT一起作用于损失函数，通过点的注记可以计算出GT。
该辅助网络仅在训练期间使用，在测试期间将被拆除。辅助网络分支与主网络分支共享主干，以提高MobilenetV2块在训练时的特征提取能力，
这类似于多任务学习的思想。也正因为如此，辅助网络可以直接使用常规卷积，而不是深度可分离的卷积。

数据不平衡样本类别分布的不平衡会导致样本量小的类别中特征过少，难以从中提取有用的信息。即使得到了模型，也容易过分依赖有限的数据样本而导致过拟合。
我们在建库的过程中采取了完全不受约束的方法，所以也存在不平衡的现象，主要体现在手掌的角度上，根据手掌角度（0度表示手掌直立的状态）分为四类。
1类对应于-45度至45度的范围（22827），2类对应于45度至135度的范围（3732），3类对应于-135度至-45度的范围（481），4类对应于剩余的范围（3178）。
针对这种情况，我们采用了数据增强的方法，比如对原始图像进行随机旋转和偏移。
此外，我们还对损失函数进行了改进，以增加批次中样本总数较少的类别所造成的损失权重。

定位关键点之后，可以提取ROI。
食指和中指之间的谷P3以及无名指和小指之间的谷P9确定坐标系的X轴。
穿过中指和无名指之间的谷P6并且垂直于X轴的线被定义为Y轴，其在原点O处与X轴相交，并且点C被定义为位于Y轴和ROI的中心两者上的点。
从P0到P12的距离d不会随着手掌的变形而发生很大变化，因此将其视为参考距离。
kOCk为11 scin24 d，ROI的边长为2 scin3 d。对于DeepMPV+，他们使用训练有素的检测器来获得两个双指间隙的中心点A和B。
点A是P2 P3的中点，而点B是P3 P4的中点。点P2、P3和P4是三个谷点，分别对应于XJTU-UP中的P3、P6和P9。
人们可能会想，ROI提取实际上只使用了5个关键点，那么在前面的过程中检测所有14个关键点的必要性是什么呢？事实上，原因与之前的工作中所述的相同。
估计额外的点可以为关键点检测提供更多的几何约束，这使得所需的五个关键点的位置更加准确和稳定。

_________________
[4_Experimental Results]()

4.1手掌检测的实现细节和评测在训练检测器时，我们需要提供包围盒的位置信息作为监督，包括中心点的坐标和盒子的宽度和高度。
为了避免手工标注包围盒，我们将14个关键点的坐标平均作为包围盒的中心坐标，然后求出所有关键点在水平和垂直坐标上的最大值和最小值，并分别求出宽度和高度。
将这两个数字乘以1.2作为边界框的宽度和高度。最后，利用得到的包围盒训练检测模型。
对于DeepMPV+，我们完全按照他们的论文生成了训练用的标注。
在训练集和测试集的划分上，我们将每部移动的手机采集的所有图片作为测试集，其余四部移动的手机对应的图片作为训练集。
我们已经训练了五个检测器，并在不同IOU阈值下测试了它们的mAP。
我们选择在IOU阈值为0.4时使用输出图像作为下一阶段的输入，并统计每个移动的手机数据集中的漏检数量。

**我们在AlexeyAB提供的暗网框架上训练了Tiny-Yolo v 3**，该框架可在Github存储库中获得：https://github.com/AlexeyAB/darknet
具有较高的mAP值和较低的误检率，并且在IOU阈值较高时优势更加明显。
我们的方法在检测方面具有高度的鲁棒性:
<ol>
    <li>一方面，这是因为我们的方法在进行手掌检测时将手掌作为一个整体，单个目标的检测难度要小得多。</li>
    <li>另一方面，与传统方法相比，基于YOLO的目标检测方法表现出了上级的性能。</li>
</ol>

关键点检测的实现细节和评估从原始手掌图像中提取包含所有关键点的矩形区域，并将其调整为112 × 112的大小作为网络的输入。
为了增强学习CNN的鲁棒性，我们进行了数据增强，如图像平移和旋转，并获得了原始训练数据量的10倍。
除了图片外，还需要输入关键点坐标值和手掌角度作为监督。坐标值被归一化，即水平和垂直坐标分别除以图片的宽度和高度。
手掌的角度也是根据关键点的坐标来计算的，取值范围是-180度到180度。

掌纹识别的实现细节及评价在得到关键点坐标的归一化值后，将横、纵坐标值分别乘以原始图像的宽、高，
**建立坐标系，提取相关区域，调整为224 × 224这是我们稍后将用于掌纹识别的ROI。**
为了比较ROI的识别准确性，我们采用了深度Meta度量学习方法。
每个移动的手机在闪光灯打开或关闭的情况下都会重复收集对象，因此总共有10个子数据库用于识别。
我们使用HF和HN分别表示华为移动的手机在闪光灯和自然光条件下采集的子数据库。IH、IN、LH、LN等具有类似的含义。
**对于每个子数据库，我们使用从所有左侧图像获得的ROI进行训练，并使用右侧ROI进行测试。**


评估掌纹识别的一些指标描述如下：
<ol>
    <li>Top-1准确率：对于测试集中的每个类别，我们随机挑选一个ROI到配准集中，并将其余的添加到查询集中。
        对于查询集中的每一幅图像，将配准集中相似度最高的图像ID作为查询图像ID，最终计算准确率。</li>
    <li>等错误率（EER）：存在一个特定的阈值，在该阈值下，错误拒绝率等于FAR。低于该阈值的错误率称为EER。</li>
    <li>低FAR下的TAR：在对风险和错误没有容忍度的情况下，探索不发生“重大错误”（概率可能是0.1%甚至0.01%）的TAR是非常必要的。</li>
</ol>
